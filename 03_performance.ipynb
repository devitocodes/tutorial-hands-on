{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance optimization and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will:\n",
    "\n",
    "* learn to optimize the performance of an `Operator`,\n",
    "* investigate the effects of optimization in two real-life seismic inversion `Operator`s,\n",
    "* analyze and interpret the performance report displayed after a run.\n",
    "\n",
    "We will rely on preset models and `Operator`s from a seismic inversion problem based on an **isotropic acoustic wave equation**. To run one such `Operator`, in particular a forward modeling operator, we will exploit the `benchmark.py` module. This provides a number of options to configure the simulation and to try out different optimizations. The `benchmark.py` is intended to let newcomers play with Devito -- and its performance optimizations! -- without having to know anything about its symbolic language, mechanisms and functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import examples.seismic.benchmark\n",
    "benchmark = examples.seismic.benchmark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a full list of options\n",
    "%run $benchmark --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we want Devito to run this `Operator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run $benchmark run -P acoustic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple. Of course, we may want to run the same simulation on a bigger grid, with different grid point spacing or space order, and so on. And yes, we'll do this later on in the tutorial. But before scaling up in problem size, we shall take a look at what sort of performance optimizations we'll be able to apply to speed it up.\n",
    "\n",
    "In essence, there are four knobs we can play with to maximize the `Operator` performance (or to see how the performace varies when adding or removing specific transformations):\n",
    "\n",
    "* parallelism,\n",
    "* the Devito Symbolic Engine (DSE) optimization level,\n",
    "* the Devito Loop Engine (DLE) optimization level,\n",
    "* loop blocking auto-tuning.\n",
    "\n",
    "## Shared-memory parallelism\n",
    "\n",
    "We start with the most obvious -- parallelism. Devito implements shared-memory parallelism via OpenMP. To enable it, the following environment variable needs to be set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env DEVITO_OPENMP=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple threads will now be used when running an `Operator`. But how many? And how efficiently? We may be running on a multi-socket node -- how should we treat it, as a \"flatten system\" or what?\n",
    "\n",
    "Devito aims to use distributed-memory parallelism over multi-socket nodes; that is, it allocates one MPI process per socket, and each MPI process should spawn as many OpenMP threads as the number of cores on the socket. Users don't get all this for free, however; a minimal configuration effort is required. But don't worry: as you shall see, it's much simpler than it sounds!\n",
    "\n",
    "For this tutorial, we forget about MPI; we rather focus on enabling OpenMP on a single socket. So, first of all, we want to restrain execution to a single socket -- we want threads to stay on that socket without ever migrating to other cores of the system due to OS scheduling. Are we really on a multi-socket node? And how many cores does a socket have? Let's find out. We shall use a very standard tool such as `lscpu` on Linux systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line beginning with `'NUMA node...'` represents one specific socket. Its value (on the right hand side, after the ':') indicates the ID ranges of its logical cores. For example, if our node consisted of two sockets, each socket having 8 physical cores and 2 hyperthreads per core, we would see something like\n",
    "\n",
    "```\n",
    "...\n",
    "NUMA node0 CPU(s):     0-7,16-23\n",
    "NUMA node1 CPU(s):     8-15,24-31\n",
    "...\n",
    "```\n",
    "\n",
    "Now, say we choose to run on the 8 cores of socket 0 (``node0``). We then simply have to set the following OpenMP environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, 8 threads will be spawned each time an `Operator` is run. They will be killed as soon as the `Operator` has done its job. \n",
    "\n",
    "We also want to **bind** them to the physical cores of socket 0; that is, we want to prevent OS-induced migration. This is known as *thread pinning*. One may use a program such as ``numactl`` or, alternatively, exploit other OpenMP environment variables. If the Intel compiler is at our disposal, we can also enforce pinning through the following two-step procedure:\n",
    "\n",
    "* We ask Devito to use the Intel compiler through the special `DEVITO_ARCH` environment variable;\n",
    "* We set the Intel-specific `KMP_HW_SUBSET` and `KMP_AFFINITY` environment variables.\n",
    "\n",
    "Let's see how we can do this in practice, and what's the impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the isotropic acoustic forward operator again, but this time with a much larger grid, a 512x512x512 cube, and a more realistic space order, 8. We also shorten the duration by deliberately choosing a very small simulation end time (50 ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thread pinning\n",
    "%env KMP_HW_SUBSET=8c,1t\n",
    "%env KMP_AFFINITY=compact\n",
    "# Tell Devito to use the Intel compiler\n",
    "%env DEVITO_ARCH=intel\n",
    "# or, equivalently, programmatically\n",
    "from devito import configuration\n",
    "configuration['compiler'] = 'intel'\n",
    "\n",
    "for i in range(3):\n",
    "    print (\"Run %d\" % i)\n",
    "    %run $benchmark run -P acoustic -so 8 -d 256 256 256 --tn 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: the execution times are stable. This is a symptom that thread pinning is working. In practice, don't forget to check by taking a look at OpenMP reports or using profilers (e.g., Intel VTune) or through user-friendly tools such as `htop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSE - Devito Symbolic Engine\n",
    "\n",
    "We know how to switch on parallelism. So, it's finally time to see what kind of optimizations can Devito perform. By default, Devito aggressively optimizes all `Operator`s. When running through `benchmark.py`, however, optimizations are left disabled until users explicitly request them. This, hopefully, simplifies initial experimentation and investigation.\n",
    "\n",
    "Let's then dive into to the Devito Symbolic Engine (or DSE) section of this tutorial. It is worth observing that the DSE is one of the distinguishing features of Devito w.r.t. many other stencil frameworks! Why is that? This is what the documentation says about the DSE:\n",
    "\n",
    "> [The DSE performs] Flop-count optimizations - They aim to reduce the operation count of an Operator. These include, for example, code motion, factorization, and detection of cross-stencil redundancies. The flop-count optimizations are performed by routines built on top of SymPy.\n",
    "\n",
    "So the DSE reduces the flop count of `Operator`s. This is particularly useful in the case of complicated PDEs, for example those making extensive use of differential operators. And even more important in high order methods. In such cases, it's not unusual to end up with kernels requiring hundreds of arithmetic operations per grid point calculation. Since Devito doesn't make assumptions about the PDEs, the presence of an optimization system such as the DSE becomes of fundamental importance. In fact, we know that its impact has been remarkable in real-life siesmic inversion operators that have been written on top of Devito (e.g., TTI operators).\n",
    "\n",
    "Let's see what happens enabling the DSE in our acoustic operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Increase Devito logger verbosity to display useful information about the DSE\n",
    "%env DEVITO_LOGGING=DEBUG\n",
    "\n",
    "run $benchmark run -P acoustic -so 8 -d 256 256 256 --tn 50 -dse advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous runs, do you note any change ...\n",
    "\n",
    "* in the Devito output reports?\n",
    "* in execution times? why?\n",
    "\n",
    "And why, from a performance analysis point of view, is the DSE useful even though no changes in execution times are observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLE - Devito Loop Engine\n",
    "\n",
    "We know how to switch on parallelism and how to leverage the DSE to reduce the flop count of our stencil equations. What's still missing are SIMD vectorization and optimizations for data locality. We won't be able to reach a significant fraction of the attainable machine peak without aggressive loop transformations. Clearly, Devito users don't \"see\" loops -- in fact, they only write maths in Python! -- but the generated code is nothing more than classic C with loop nests for grid function updates. So how do these \"low-level loops\" get optimized? Quoting from the documentation:\n",
    "\n",
    "> Loop optimizations - Examples include SIMD vectorization and loop blocking. These are performed by the Devito Loop Engine (DLE), a sub-module consisting of a sequence of compiler passes manipulating abstract syntax trees [...]\n",
    "\n",
    "In other words, the Devito compiler, through the DLE module, automatically applies loop transformations. The **how it does that**(i.e., manipulation of an intermediate representation), here, is irrelevant; we rather want to understand **what the DLE can do**, **how to use** it and what kind of **tuning** is required to maximize the performance of an `Operator`. As we shall see, using and tuning the DLE will be as simple as switching on some special environment variables!\n",
    "\n",
    "So here's a (non complete) list of transformations that the DLE will automatically apply for you:\n",
    "\n",
    "* SIMD Vectorization\n",
    "* Loop blocking\n",
    "* Optimization of so called \"remainder\" loops\n",
    "* Creation of elemental functions\n",
    "\n",
    "Before enabling the DLE, let's first run a problem bigger than those tried in the previous sections. We pick a 512\\*\\*3 grid to be 100% sure that the working set of our simulation won't fit in some level of cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run $benchmark run -P acoustic -so 8 -d 512 512 512 --tn 50 -dse advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's run the same problem with the DLE at maximum level (it's gonna apply **all** optimizations listed above). Can you guess whether the performance will be substantially better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run $benchmark run -P acoustic -so 8 -d 512 512 512 --tn 50 -dse advanced -dle advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make the `Operator` run quicker? Yes !\n",
    "\n",
    "What we are missing so far is performance tuning. Take loop blocking, for example. This is a *parametric loop transformation*: its impact will vary depending on block shape, size and scheduling. In the literature, over the years, dozens of different loop blocking strategies have been studied! Even though we used the simplest loop blocking scheme on Earth, we would need **at least** to come up with a block size fitting in some level of cache. Obviously, this is such a low level detail... and we don't want users to waste any time on thinking about these matters!\n",
    "\n",
    "Like other frameworks, Devito can automatically detect a \"sufficiently good\" block size through an *auto-tuning engine*. Let's try it out; in particular, we tell Devito to be \"aggressive\" in the search for block sizes. Being \"aggressive\" means that more time will be spent on auto-tuning, but there's a greater chance of retrieving a better candidate. We do this through the `DEVITO_AUTOTUNING` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env DEVITO_AUTOTUNING=aggressive\n",
    "run $benchmark run -P acoustic -so 8 -d 512 512 512 --tn 50 -dse advanced -dle advanced -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the addition of `-a` to the arguments list of our benchmarking script. This enables auto-tuning; the `DEVITO_AUTOTUNING=aggressive` environment variable drives the auto-tuner search.\n",
    "\n",
    "Do you note any difference in performance? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: performance analysis of a TTI forward operator\n",
    "\n",
    "There's another operator that you can try running with our benchmarking script, namely a Tilted Transverse Isotropic operator for forward modeling. This one is even much more interesting than the isotropic acoustic one, as it's representative of a class of real-life wave modeling operators. The physics, and thus the equations, are more complicated, which results in computationally expensive numerical kernels. You can appreciate that yourself by skimming through the genereted code, whose location is displayed after JIT compilation.\n",
    "\n",
    "Here's how to run TTI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run $benchmark run -P tti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise asks you to repeat the same study we did for acoustic, playing with different DSE and DLE levels. Oh, and don't forget, there's another DSE level that you can try besides `advanced`, namely `aggressive`; just give it a go. How does the performance impact of DSE/DLE vary w.r.t. isotropic acoustic operator? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A sneak peek at the YASK backend\n",
    "\n",
    "YASK -- Yet Another Stencil Kernel -- is, as described on its GitHub page\n",
    "> a framework to facilitate exploration of the HPC stencil-performance design space.\n",
    "\n",
    "The fundamental point is that YASK operates at a level of abstraction lower than that of Devito; for example, besides being a static system written in C++, no symbolic language is offered. We've been working on integrating YASK, meaning that, under the hood, Devito may \"offload\" (some of) the `Operator` stencil updates onto YASK. **This is totally transparent to users**; no changes to existing user code are required to exploit YASK! The goal is to create a synergy between the simplicity of use of Devito (i.e., the high-level abstractions) and a powerful optimization system such as YASK, specifically conceived to optimize stencil codes.\n",
    "\n",
    "So, how do we try out YASK? Again, it's as difficult as setting one more environment variable. Watch out, however, as this is work in progress: the performance may still be quite far from the attainable peaks (e.g., Devito cannot still leverage YASK's auto-tuning system) and some `Operator`s are still unsupported, although our running example should just work out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env DEVITO_BACKEND=yask\n",
    "run $benchmark run -P acoustic -so 8 -d 512 512 512 --tn 50 -dse advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we didn't even have to specify the `-a` and `-dle advanced` options, because YASK takes responsability for loop optimization and auto-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup>This notebook is part of the tutorial \"Optimised Symbolic Finite Difference Computation with Devito\" presented at the IntelÂ® HPC Developer Conference 2017.</sup>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
